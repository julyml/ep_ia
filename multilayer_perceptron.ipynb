{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multilayer perceptron",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPuvyH1OsddkTjHvdx9UZML",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/julyml/ep_ia/blob/main/multilayer_perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyR6ABI48I1w"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class multilayer_perceptron():\n",
        "    def __init__(self,tamanho_escondida,tamanho_saida,taxa_aprendizado, ativacao,d_ativacao, bias_entrada, bias_escondida,tamanho_entrada):\n",
        "        # carrega o arquivo de dados de uma url\n",
        "        self.ativacao = ativacao\n",
        "        self.d_ativacao = d_ativacao\n",
        "        #caracterısticas extraıdas (n) de uma imagem \n",
        "        self.n = tamanho_entrada\n",
        "        # print(\"Caracteristicas n:\\n\",self.tamanho_entrada)\n",
        "\n",
        "        #numero de neuronios na camada escondida\n",
        "        self.p = tamanho_escondida  \n",
        "        # print(\"Tamanho da camada escondida p\\n\",self.p)\n",
        "\n",
        "        # numero de pesos da camada escondida é p.(n+ 1)\n",
        "        self.n_v = self.p *(self.n + 1)\n",
        "        # print(\"Número de pesos da camada escondida:\\n\",self.n_v) \n",
        "        \n",
        "        #pesos iniciais da camada escondida\n",
        "        self.V = self.init_weights_hidden()\n",
        "        self.V.to_csv('Pesos iniciais da camanda escondida.csv')\n",
        "        # print(\"Pesos iniciais da camada escondida - V\\n\",self.V)   \n",
        "\n",
        "        #numero de neuronios na camada de saida\n",
        "        self.m = tamanho_saida \n",
        "        # print(\"Tamanho da camada de saída\\n\",self.m)\n",
        "        \n",
        "        #numero de pesos na camada de saida m.(p + 1)\n",
        "        self.n_w = self.m * (self.p + 1)\n",
        "        # print(\"Numero de pesos da camada de saída\\n\",self.n_w)\n",
        "\n",
        "        #pesos iniciais da camada de saida\n",
        "        self.W = self.init_weights_output() \n",
        "        self.W.to_csv('Pesos iniciais da camanda de saida.csv')\n",
        "        # print(\"pesos iniciais da camada de saida - W\\n \", self.W)\n",
        "\n",
        "        # define um numero entre 0 e 1 como taxa de aprendizado\n",
        "        self.α = taxa_aprendizado\n",
        "        # print(\"Taxa de aprendizado\\n\",self.α)\n",
        "\n",
        "        # define o valor do bias na camada de entrada\n",
        "        self.bias_entrada = bias_entrada\n",
        "        # print(\"Bias da camada de entrada\\n\", self.bias_entrada)\n",
        "\n",
        "        # define o valor do bias na camada escondida\n",
        "        self.bias_escondida = bias_escondida\n",
        "        self.bias_escondida =  pd.DataFrame([bias_escondida])\n",
        "        # print(\"Bias da camada escondida\\n\", self.bias_escondida)\n",
        "\n",
        "        parametros = [\n",
        "                      ['Caracteristicas extraídas',self.n],\n",
        "                      ['Tamanho da camada escondida',self.p],\n",
        "                      ['Número de pesos da camada escondida',self.n_v],\n",
        "                      ['Tamanho da camada de saída',self.m],\n",
        "                      ['Numero de pesos da camada de saída',self.n_w],\n",
        "                      ['Taxa de aprendizado',self.α],\n",
        "                      ['Bias da camada de entrada',self.bias_entrada],\n",
        "                      ['Bias da camada escondida',self.bias_escondida]]    \n",
        "\n",
        "        pd.DataFrame(data=parametros, columns=['parametro','valor']).to_html('parametros_iniciais.html')\n",
        "\n",
        "    def init_weights_hidden(self): \n",
        "      #retorna uma matriz com pesos aleatorios para camada escondida\n",
        "      return pd.DataFrame(np.random.rand(self.p,(self.n+1)))\n",
        "\n",
        "    def init_weights_output(self): \n",
        "      #inicializa os pesos da camada de saida\n",
        "      return pd.DataFrame(np.random.rand(self.m,(self.p+1))) \n",
        "\n",
        "    def feedforward(self,  row): \n",
        "      '''\n",
        "      Passos 3, 4 e 5 do algoritmo da pagina 294 do livro da Faussett\n",
        "      '''\n",
        "      #row ja vem com o bias \n",
        "      row = pd.DataFrame(row)     \n",
        "      # print(\"X:\\n\",row)      \n",
        "      # print('V:\\n',self.V)\n",
        "      # print('W:\\n',self.W)\n",
        "\n",
        "      Z_in = self.V.dot(row) \n",
        "      Z_in.columns = range(Z_in.shape[1])\n",
        "      # print(\"Z_in:\\n\",Z_in)\n",
        "\n",
        "      #aplica a funcao de ativação sobre o dados da camada escondida\n",
        "      Z = Z_in.applymap(self.ativacao)\n",
        "      Z.columns = range(Z.shape[1])\n",
        "      # print(\"Z:\\n\",Z)\n",
        "\n",
        "      # adicionamos o bias da camada escondida  = z chapeu invertido\n",
        "      Z_bias_escondida =  pd.concat([self.bias_escondida, Z]).reset_index(drop = True)\n",
        "      Z_bias_escondida.columns = range(Z_bias_escondida.shape[1])\n",
        "      # print('Z_bias_escondida\\n',Z_bias_escondida)      \n",
        "\n",
        "      # calculamos o valor de Y inativado\n",
        "      Y_in = self.W.dot(Z_bias_escondida)\n",
        "      Y_in.columns = range(Y_in.shape[1])\n",
        "      # print('Y_in:\\n',Y_in)\n",
        "\n",
        "      # #aplica a funcao de ativação sobre o dados da camada de saida (Y_in)\n",
        "      Y = Y_in.apply(self.ativacao)\n",
        "      Y.columns = range(Y.shape[1])\n",
        "      # print('Y:\\n',Y)\n",
        "      return Z_in,Z_bias_escondida,Z,Y_in,Y\n",
        "\n",
        "    def backpropagation(self,Z_in,Z_bias_escondida,Z,Y_in,Y,tk,xk):\n",
        "      # Camada de saida          \n",
        "          # calculo a taxa de erro ou seja o esperado menos o recebido\n",
        "        # aplica a sigmoide derivada ao Y_in\n",
        "        Y_D = Y_in.apply(self.d_ativacao) \n",
        "        # print('Y_D:\\n',Y_D)\n",
        "\n",
        "        # calculo da informaçao de erro\n",
        "        δs = (tk-Y) * Y_D\n",
        "        # print('δs\\n',δs)\n",
        "\n",
        "        # calculo do termo de correçao\n",
        "        Δw = list()\n",
        "        for i, δsi in δs.iterrows():\n",
        "          Δw.append((self.α * δsi * Z_bias_escondida).T)\n",
        "        Δw = pd.concat(Δw,ignore_index=True)\n",
        "        # print('Δw\\n',Δw)\n",
        "        \n",
        "        '''\n",
        "          Passo 7 do algoritmo descrito na página 294 do livro da Fausset\n",
        "        '''\n",
        "\n",
        "        # Camada escondida HIDDEN\n",
        "          # calculo a taxa de erro ou seja o esperado menos o recebido          \n",
        "        # aplica a sigmoide derivada ao Z_in          \n",
        "        Z_D = Z_in.apply(self.d_ativacao)  \n",
        "        # print('Z_D:\\n',Z_D)\n",
        "\n",
        "        #calculo da informaçao de erro\n",
        "        δh = pd.DataFrame()\n",
        "        w_without_first_Columns = self.W.iloc[:, 1:] # mas por que eu ignorei a primeira coluna\n",
        "        w_without_first_Columns.columns = range(w_without_first_Columns.shape[1])\n",
        "\n",
        "        for wii in w_without_first_Columns:\n",
        "          w_temp = pd.DataFrame(w_without_first_Columns[wii])\n",
        "          w_temp.columns = range(w_temp.shape[1])\n",
        "          δhi =  Z_D.iloc[wii] * (δs * w_temp).sum()\n",
        "          δh = δh.append([δhi],ignore_index=True)          \n",
        "        # print('δh:\\n',δh)\n",
        "\n",
        "        # calculo do termo de correçao\n",
        "        Δv_lista = list()\n",
        "        for i, δhi in δh.iterrows():\n",
        "          xk = pd.DataFrame(xk)    \n",
        "          xk.columns = range(xk.shape[1])\n",
        "          Δv_lista.append((self.α * δhi * xk).T)\n",
        "        Δv = pd.concat(Δv_lista,ignore_index=True)\n",
        "        # print('Δv\\n',Δv)\n",
        "       \n",
        "        return δs,Δw,δh,Δv\n",
        "    \n",
        "    def train(self, dados,threshold = 1e-3):\n",
        "      \n",
        "      targets = dados.iloc[:,self.n:]\n",
        "      targets.columns = range(targets.shape[1])\n",
        "\n",
        "      inputs = dados.iloc[: , :self.n]\n",
        "      inputs.insert(inputs.shape[0]+1, \"bias\", self.bias_entrada)\n",
        "      inputs.columns = range(inputs.shape[1])        \n",
        "\n",
        "      square_error = 2 * threshold\n",
        "      counter = 1\n",
        "      δ_list_ = []\n",
        "      while square_error > threshold:        \n",
        "        square_error = 0\n",
        "        for (i, input), (j, target)  in zip(inputs.iterrows(),targets.iterrows()):\n",
        "          # entrada do treino = X\n",
        "          xk = input.reset_index(drop=True) \n",
        "          # saida esperada para a entrada acima = Target \n",
        "          tk = pd.DataFrame(target).reset_index(drop=True) \n",
        "          tk.columns = range(tk.shape[1])\n",
        "\n",
        "          # Passo os dados de input (Xk) pelo passo feed\n",
        "          Z_in,Z_bias_escondida,Z,Y_in,Y = self.feedforward(xk) \n",
        "\n",
        "          # calculo a taxa de erro usando o Y\n",
        "          error = tk.reset_index(drop=True)-Y\n",
        "          square_error = (square_error + error.pow(2).sum())[0]\n",
        "          # square_error = square_error + sum(error*2)\n",
        "          '''\n",
        "           Passo 6 do algoritmo descrito na página 294 do livro da Fausset\n",
        "          '''\n",
        "          δs,Δw,δh,Δv = self.backpropagation(Z_in,Z_bias_escondida,Z,Y_in,Y,tk,xk)   \n",
        "\n",
        "          δ_list_.append([counter,δh,δs])\n",
        "\n",
        "          '''\n",
        "           Passo 8 do algoritmo descrito na página 294 do livro da Fausset\n",
        "          '''\n",
        "          #mudar os pesos das camadas\n",
        "          self.W =  self.W.add(Δw, fill_value=0)\n",
        "          self.V = self.V.add(Δv, fill_value=0)\n",
        "\n",
        "        square_error = square_error / len(dados.index)\n",
        "        # print('Erro médio quadrado:\\n',square_error)\n",
        "        counter+=1      \n",
        "\n",
        "      self.V.to_csv('Pesos finais da camanda de saida.csv')\n",
        "      self.W.to_csv('Pesos finais da camanda de saida.csv')\n",
        "      header = ['iteracao','erros escondida', 'erros saida']\n",
        "      pd.DataFrame(δ_list_, columns=header).to_csv(f'Erros das camandas.csv')\n",
        "      # print('Quantidade de iterações do treino:\\n',counter)\n",
        "      return 'Rede treinada com sucesso'\n",
        "\n",
        "    def predict(self, xk):\n",
        "      # adiciono o bias a entrada de teste\n",
        "      xk = xk.append(pd.Series([self.bias_entrada]),ignore_index=True)\n",
        "      _,_,_,_,Y = self.feedforward(xk)\n",
        "      return Y\n",
        "\n",
        "    def test_dataset(self, dados):\n",
        "      targets = dados.iloc[:,self.n:]\n",
        "      targets.columns = range(targets.shape[1])\n",
        "\n",
        "      inputs = dados.iloc[: , :self.n]\n",
        "      inputs.columns = range(inputs.shape[1])   \n",
        "\n",
        "      outputs = list()\n",
        "\n",
        "      for (i, input), (j, target)  in zip(inputs.iterrows(),targets.iterrows()):        \n",
        "        input = input.reset_index(drop=True)\n",
        "        tk = pd.DataFrame(target).reset_index(drop=True)\n",
        "        tk.columns = range(tk.shape[1])\n",
        "        result = self.predict(input)[0].to_list()\n",
        "        outputs.append([result,target[0].to_list() ])\n",
        "      \n",
        "      return outputs\n",
        "\n",
        "    def matriz_confusao_caracteres(self,outputs):\n",
        "      # output[0]: lista de lista com todos os outputs reais de teste\n",
        "      # output[1]: lista de lista de todos os targets de teste    \n",
        "\n",
        "      lista_letras = ['A','B','C','D','E','F','G']\n",
        "      target_list = ['[1,0,0,0,0,0,0]','[0,1,0,0,0,0,0]','[0,0,1,0,0,0,0]','[0,0,0,1,0,0,0]','[0,0,0,0,1,0,0]','[0,0,0,0,0,1,0]','[0,0,0,0,0,0,1]']\n",
        "\n",
        "      dicionario_letras = dict(zip(target_list,lista_letras))\n",
        "      \n",
        "      #cria uma matriz do tamanho do caracteres para criar a matriz de confusão multiclasse\n",
        "      matrix = pd.DataFrame(0, index=lista_letras, columns=lista_letras)\n",
        "\n",
        "      for output in outputs:\n",
        "        out = [int(round(x,1)) for x in output[0]]\n",
        "        chave_linha = str(out).replace(' ', '')\n",
        "        chave_coluna = str(output[1]).replace(' ', '')\n",
        "        linha = dicionario_letras[chave_linha]\n",
        "        coluna = dicionario_letras[chave_coluna]      \n",
        "        matrix.at[linha,coluna]=matrix.at[linha,coluna]+1\n",
        "      return matrix\n",
        "\n",
        "    def generate_matriz_confusao(self,dados):\n",
        "      outputs = self.test_dataset(dados)\n",
        "      return self.matriz_confusao_caracteres(outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1W5mLFdIb3-"
      },
      "source": [
        "def sigmoide(n):\n",
        "  #Funcao de ativaçao sigmoide\n",
        "  return  1.0 / (1.0 + np.exp(-n))\n",
        "\n",
        "def derivada_sigmoide(n):       \n",
        "  #Funcao derivada da sigmoide\n",
        "  return sigmoide(n) * (1 - sigmoide(n))  \n",
        "\n",
        "\n",
        "mlp = multilayer_perceptron(tamanho_escondida=15,\n",
        "                            tamanho_saida=7,\n",
        "                            taxa_aprendizado=0.5,                            \n",
        "                            ativacao = sigmoide,\n",
        "                            d_ativacao = derivada_sigmoide,\n",
        "                            bias_entrada=1,\n",
        "                            bias_escondida=1,\n",
        "                            tamanho_entrada=63)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_vYEgk9ljpB"
      },
      "source": [
        "dataset_url = 'https://raw.githubusercontent.com/julyml/ep_ia/main/Dados/caracteres-limpo.csv'\n",
        "\n",
        "dados = pd.read_csv(dataset_url, header=None,delimiter=',') \n",
        "\n",
        "msk = np.random.rand(len(dados)) < 0.8\n",
        "\n",
        "treino = dados[msk]\n",
        "\n",
        "teste = dados[~msk]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5ab1m2qiOJu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "4bbceea0-42bf-44da-defd-7306ceba8f21"
      },
      "source": [
        "mlp.train(treino)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Rede treinada com sucesso'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxOx0uY6fgIc"
      },
      "source": [
        "mlp.predict(pd.Series([-1,-1,1,1,-1,-1,-1,-1,-1,-1,1,-1,-1,-1,-1,-1,-1,1,-1,-1,-1,-1,-1,1,-1,1,-1,-1,-1,-1,1,-1,1,-1,-1,-1,1,1,1,1,1,-1,-1,1,-1,-1,-1,1,-1,-1,1,-1,-1,-1,1,-1,1,1,1,-1,1,1,1]).T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Msx2pq5eOC7"
      },
      "source": [
        "mlp.generate_matriz_confusao(teste)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}